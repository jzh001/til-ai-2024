# DSTA TIL AI 2024 - Team Anti Heroes
## Advanced Track - Champion


## Natural Language Processing
### Goal
Given a transcript, extract the heading, tool and target.
### Solution
Classical Approach:
1. Heading
   Use a sliding window to identify a chain of consecutive numeric words (eg. "one two two") and convert them to integer format.
2. Tool
   Use a sliding window to extract fixed phrases for the tool, based on the tools used in the training set.
3. Target
   Use a sliding window to identify attributes (eg. colors like "red") and nouns (eg "jet"), and find word patterns in the form of attribute + XXX + noun, where XXX can be any sequence of words.

Transformer Question Answering Approach:
This approach is used when the above classical approach fails. We fine-tune a pretrained RoBERTa Question Answering model that is fine-tuned on the SQuAD2.0 Dataset, to answer questions about the heading, tool and target. (deepset/roberta-base-squad2)

Below are the 3 questions asked:

- What is the heading?
- What is the tool to deploy?
- What is the target to engage?

Overall Test Score:
- Accuracy 0.99933333
- Speed 0.84758943

## Automatic Speech Recognition (ASR)
### Goal
Given a WAV audio file, return the text transcript in a string.
### Solution
We fine-tuned a Whisper Small model on the audio dataset for 4 epochs. (openai/whisper-small)

Overall Test Score:
- Accuracy 0.99516705
- Speed 0.80086243

## Vision Language Model (VLM)
### Goal
Given an image-caption pair, draw a bounding box around the object described by the caption.
### Solution
We broke this task down into two parts:
1. Object Detection
2. Image-Text Model to score an image-caption pair

#### Object Detection
We used the Real-Time Detection Transformer (RT-DETR) by Ultralytics, developed by Baidu, and trained for 15 epochs. We used the default RT-DETR augmentations.

#### Image Captioning
We developed an ensemble of two models by finding the average of the logits generated by 4 models:
1. Vision Text Dual Encoder Model comprising
   - Google Vision Transformer (ViT) - google/vit-base-patch16-224
   - RoBERTa - FacebookAI/roberta-base
2. Vision Text Dual Encoder Model comprising
   - BERT Pre-Training of Image Transformers (BEiT) - microsoft/beit-base-patch16-224-pt22k-ft22k
   - RoBERTa - FacebookAI/roberta-base
3. Vision Text Dual Encoder Model comprising
   - BERT Pre-Training of Image Transformers (BEiT) - microsoft/beit-base-patch16-224-pt22k-ft22k
   - Supervised RoBERTa SimCSE - princeton-nlp/sup-simcse-roberta-base
4. Vision Text Dual Encoder Model comprising
   - Google Vision Transformer (ViT) pretrained on planes - Illia56/Illia56-Military-Aircraft-Detection
   - MPNet - sentence-transformers/all-mpnet-base-v2

Overall Test Score:
- Accuracy 0.863
- Speed 0.76421

## References
Data and Sample Notebooks provided by Defence Science and Technology Agency (DSTA), Singapore, under BrainHack 2024: Today I Learnt - Artificial Intelligence (TIL-AI). 
Notebooks and code were also referenced heavily from the Huggingface libraries and documentations.
