{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e756cb8-5c31-4f6c-a37a-4f8afd2e0c3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.30.1)\n",
      "Requirement already satisfied: transformers==4.37.0 in /opt/conda/lib/python3.10/site-packages (4.37.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.19.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.37.0) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers==4.37.0) (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.37.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.37.0) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.37.0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.37.0) (2024.5.10)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.37.0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers==4.37.0) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.37.0) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.37.0) (4.66.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.3.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.0) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.37.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.37.0) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.37.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.37.0) (2023.7.22)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate transformers==4.37.0 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41481d29-59f2-44cc-8b8d-0affb815d846",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The projection layer and logit scale weights `['visual_projection.weight', 'text_projection.weight', 'logit_scale']` are newly initialized. You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from torchvision.io import ImageReadMode, read_image\n",
    "from torchvision.transforms import CenterCrop, ConvertImageDtype, Normalize, Resize, RandomHorizontalFlip,RandomAffine,GaussianBlur,RandomCrop, RandomRotation, RandomResizedCrop, CenterCrop\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    VisionTextDualEncoderModel,\n",
    "    VisionTextDualEncoderProcessor,\n",
    "    AutoTokenizer,\n",
    "    AutoImageProcessor\n",
    ")\n",
    "\n",
    "model = VisionTextDualEncoderModel.from_vision_text_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\", \"princeton-nlp/sup-simcse-roberta-base\"\n",
    ")\n",
    "#facebook/deit-base-distilled-patch16-384\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"princeton-nlp/sup-simcse-roberta-base\")\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "processor = VisionTextDualEncoderProcessor(image_processor, tokenizer)\n",
    "config = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6a241e9-2223-4fba-aa29-47cba8833f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image_path', 'image_id', 'original_id', 'caption'],\n",
       "        num_rows: 26517\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image_path', 'image_id', 'original_id', 'caption'],\n",
       "        num_rows: 1396\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['image_path', 'image_id', 'original_id', 'caption'],\n",
       "        num_rows: 1396\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "file_path = \"./vlm_caption.jsonl\"\n",
    "dataset = load_dataset('json', data_files=file_path)\n",
    "dataset = dataset['train'].train_test_split(test_size=0.05, seed = 42)\n",
    "dataset['valid'] = dataset['test'] \n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc8453e3-63e0-4d6a-8e21-538c1937f8fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_path': './cropped/40200.jpg',\n",
       " 'image_id': 40200,\n",
       " 'original_id': 2010,\n",
       " 'caption': 'white and orange light aircraft'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90f90368-0a0a-46fa-8290-fff0e362df89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use torchvision for faster image pre-processing. The transforms are implemented as nn.Module,\n",
    "# so we jit it to be faster.\n",
    "class Transform(torch.nn.Module):\n",
    "    def __init__(self, image_size, mean, std):\n",
    "        super().__init__()\n",
    "        self.transforms = torch.nn.Sequential(\n",
    "            #Resize([image_size + 5], interpolation=InterpolationMode.BICUBIC),\n",
    "            #RandomCrop(image_size),\n",
    "            RandomResizedCrop(image_size, scale=(0.9,1.0),interpolation=InterpolationMode.BICUBIC),\n",
    "            ConvertImageDtype(torch.float),\n",
    "            RandomRotation(10),\n",
    "            RandomHorizontalFlip(p=0.5),\n",
    "            #GaussianBlur(kernel_size=3),\n",
    "            Normalize(mean, std),\n",
    "        )\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        \"\"\"`x` should be an instance of `PIL.Image.Image`\"\"\"\n",
    "        with torch.no_grad():\n",
    "            x = self.transforms(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class TestTransform(torch.nn.Module):\n",
    "    def __init__(self, image_size, mean, std):\n",
    "        super().__init__()\n",
    "        self.transforms = torch.nn.Sequential(\n",
    "            Resize([image_size], interpolation=InterpolationMode.BICUBIC),\n",
    "            CenterCrop(image_size),\n",
    "            #RandomResizedCrop(image_size, scale=(0.8,1.0),interpolation=InterpolationMode.BICUBIC),\n",
    "            ConvertImageDtype(torch.float),\n",
    "            #RandomRotation(90),\n",
    "            #RandomHorizontalFlip(p=0.5),\n",
    "            #GaussianBlur(kernel_size=3),\n",
    "            Normalize(mean, std),\n",
    "        )\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        \"\"\"`x` should be an instance of `PIL.Image.Image`\"\"\"\n",
    "        with torch.no_grad():\n",
    "            x = self.transforms(x)\n",
    "        return x\n",
    "\n",
    "# For preprocessing the datasets.\n",
    "# Initialize torchvision transforms and jit it for faster processing.\n",
    "transform = Transform(\n",
    "    config.vision_config.image_size, image_processor.image_mean, image_processor.image_std\n",
    ")\n",
    "transform = torch.jit.script(transform)\n",
    "\n",
    "test_transform = TestTransform(\n",
    "    config.vision_config.image_size, image_processor.image_mean, image_processor.image_std\n",
    ")\n",
    "test_transform = torch.jit.script(test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33d901a5-97cc-4bc7-a7b9-9c9fd5772fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset, split):\n",
    "    # Preprocessing the datasets.\n",
    "    data = dataset[split]\n",
    "    # We need to tokenize inputs and targets.\n",
    "    column_names = data.column_names\n",
    "\n",
    "    # 6. Get the column names for input/target.\n",
    "    image_column = \"image_path\"\n",
    "    caption_column = \"caption\"\n",
    "    dataset_columns = (image_column, caption_column)\n",
    "\n",
    "    # Preprocessing the datasets.\n",
    "    # We need to tokenize input captions and transform the images.\n",
    "    def tokenize_captions(examples):\n",
    "        captions = list(examples[caption_column])\n",
    "        text_inputs = tokenizer(captions, padding=\"max_length\", truncation=True)\n",
    "        examples[\"input_ids\"] = text_inputs.input_ids\n",
    "        examples[\"attention_mask\"] = text_inputs.attention_mask\n",
    "        return examples\n",
    "\n",
    "    def transform_images(examples):\n",
    "        images = [read_image(image_file, mode=ImageReadMode.RGB) for image_file in examples[image_column]]\n",
    "        if split == 'train':\n",
    "            examples[\"pixel_values\"] = [transform(image) for image in images]\n",
    "        else:\n",
    "            examples[\"pixel_values\"] = [test_transform(image) for image in images]\n",
    "        return examples\n",
    "\n",
    "    data = data.map(\n",
    "        function=tokenize_captions,\n",
    "        batched=True,\n",
    "        remove_columns=[col for col in column_names if col != image_column],\n",
    "        desc=f\"Running tokenizer on {split} dataset\",\n",
    "    )\n",
    "\n",
    "    # Transform images on the fly as doing it on the whole dataset takes too much time.\n",
    "    data.set_transform(transform_images)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b43b9107-6368-46e1-8538-8a08c22c690a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'transform'=<function preprocess_dataset.<locals>.transform_images at 0x7f13db69c670> of the transform datasets.arrow_dataset.Dataset.set_format couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = preprocess_dataset(dataset, \"train\")\n",
    "eval_dataset = preprocess_dataset(dataset, \"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb9dde24-5e6f-4db1-acdc-38282d5efafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    input_ids = torch.tensor([example[\"input_ids\"] for example in examples], dtype=torch.long)\n",
    "    attention_mask = torch.tensor([example[\"attention_mask\"] for example in examples], dtype=torch.long)\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"return_loss\": True,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea68f5a8-74cd-4aa2-b0e6-082f3e1f06bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6645' max='9945' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6645/9945 3:10:02 < 1:34:24, 0.58 it/s, Epoch 2.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.094400</td>\n",
       "      <td>0.099741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.055100</td>\n",
       "      <td>0.066080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize Trainer\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=0,\n",
    "    weight_decay=0.02,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    remove_unused_columns=False,\n",
    "    output_dir=\"simcse-vit\",\n",
    "    save_strategy = \"no\",\n",
    "    num_train_epochs=3,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c3adeaf-69f2-4412-b4c9-2a98caf03e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['caption_models/simcse-vit-v1/preprocessor_config.json']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = 'caption_models/simcse-vit-v1'\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "image_processor.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44dc3af-c5dc-4dc8-ac97-f24f34b0de21",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f647ec-82f0-4984-bc03-6d36686bc075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287dd6f9-92c4-44a2-a20d-a0d600095c12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "image = [Image.open(dataset['train'][idx]['image_path'])]\n",
    "texts = [dataset['train'][idx]['caption'], \"black helicopter\", \"black fighter plane\", \"white missile\"]\n",
    "print(texts)\n",
    "inputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True).to(device)\n",
    "model = model.to(device)\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image.sigmoid() \n",
    "print(logits_per_image)\n",
    "image[0]"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
