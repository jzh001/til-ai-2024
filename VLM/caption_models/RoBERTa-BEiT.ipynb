{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "125eb28a-dc3e-4917-9e57-ae8e548adc91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from preprocessing import createCroppedFolder\n",
    "\n",
    "#createCroppedFolder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff61c338-d803-4a1e-896e-de6dcac712c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# file_path = \"./vlm_caption.jsonl\"\n",
    "# df = pd.read_json(path_or_buf=file_path, lines=True)\n",
    "# df = df.rename(columns={'image_path': 'file_name'})\n",
    "# df['file_name'] = ['./'+ name[-4:]\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e756cb8-5c31-4f6c-a37a-4f8afd2e0c3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/envs/pytorch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: accelerate in /opt/conda/envs/pytorch/lib/python3.10/site-packages (0.30.1)\n",
      "Requirement already satisfied: transformers==4.37.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (4.37.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/envs/pytorch/lib/python3.10/site-packages (2.19.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.37.0) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.37.0) (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.37.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.37.0) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.37.0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.37.0) (2024.5.10)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.37.0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.37.0) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.37.0) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.37.0) (4.66.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from accelerate) (5.9.3)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from accelerate) (2.3.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (16.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.0) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers==4.37.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers==4.37.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers==4.37.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers==4.37.0) (2024.2.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/envs/pytorch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install accelerate transformers==4.37.0 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41481d29-59f2-44cc-8b8d-0affb815d846",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The projection layer and logit scale weights `['visual_projection.weight', 'text_projection.weight', 'logit_scale']` are newly initialized. You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from torchvision.io import ImageReadMode, read_image\n",
    "from torchvision.transforms import CenterCrop, ConvertImageDtype, Normalize, Resize, RandomHorizontalFlip,RandomAffine,GaussianBlur,RandomCrop\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    VisionTextDualEncoderModel,\n",
    "    VisionTextDualEncoderProcessor,\n",
    "    AutoTokenizer,\n",
    "    AutoImageProcessor\n",
    ")\n",
    "\n",
    "model = VisionTextDualEncoderModel.from_vision_text_pretrained(\n",
    "    \"microsoft/beit-base-patch16-224-pt22k-ft22k\", \"FacebookAI/roberta-base\"\n",
    ")\n",
    "#facebook/deit-base-distilled-patch16-384\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"microsoft/beit-base-patch16-224-pt22k-ft22k\")\n",
    "processor = VisionTextDualEncoderProcessor(image_processor, tokenizer)\n",
    "config = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6a241e9-2223-4fba-aa29-47cba8833f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image_path', 'image_id', 'original_id', 'caption'],\n",
       "        num_rows: 26517\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image_path', 'image_id', 'original_id', 'caption'],\n",
       "        num_rows: 1396\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['image_path', 'image_id', 'original_id', 'caption'],\n",
       "        num_rows: 1396\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "file_path = \"./vlm_caption.jsonl\"\n",
    "dataset = load_dataset('json', data_files=file_path)\n",
    "dataset = dataset['train'].train_test_split(test_size=0.05, seed = 42)\n",
    "dataset['valid'] = dataset['test'] \n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2900a1bb-1ca5-4372-8fd0-3d1e9f4be143",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset.save_to_disk(\"saved_cropped_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4600b2b4-fe77-4f9d-9d36-6f720e5908a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from datasets import load_from_disk\n",
    "\n",
    "# dataset = load_from_disk(\"saved_cropped_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc8453e3-63e0-4d6a-8e21-538c1937f8fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_path': './cropped/40200.jpg',\n",
       " 'image_id': 40200,\n",
       " 'original_id': 2010,\n",
       " 'caption': 'white and orange light aircraft'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b567c72a-3301-455b-a585-6098b66ce597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use torchvision for faster image pre-processing. The transforms are implemented as nn.Module,\n",
    "# so we jit it to be faster.\n",
    "class Transform(torch.nn.Module):\n",
    "    def __init__(self, image_size, mean, std):\n",
    "        super().__init__()\n",
    "        self.transforms = torch.nn.Sequential(\n",
    "            Resize([image_size + 5], interpolation=InterpolationMode.BICUBIC),\n",
    "            RandomCrop(image_size),\n",
    "            ConvertImageDtype(torch.float),\n",
    "            RandomAffine((10)),\n",
    "            RandomHorizontalFlip(p=0.5),\n",
    "            #GaussianBlur(kernel_size=3),\n",
    "            Normalize(mean, std),\n",
    "        )\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        \"\"\"`x` should be an instance of `PIL.Image.Image`\"\"\"\n",
    "        with torch.no_grad():\n",
    "            x = self.transforms(x)\n",
    "        return x\n",
    "\n",
    "# For preprocessing the datasets.\n",
    "# Initialize torchvision transforms and jit it for faster processing.\n",
    "image_transformations = Transform(\n",
    "    config.vision_config.image_size, image_processor.image_mean, image_processor.image_std\n",
    ")\n",
    "image_transformations = torch.jit.script(image_transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebe44a46-0a5a-4a85-9da2-fc0cac23f0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# README\n",
    "# @zhiheng just copy this cell over to wherever, rephrase(phrase) will do the job\n",
    "# 62% of the phrases have a colour adjective, so 62% of the phrases will change\n",
    "\n",
    "import random\n",
    "random.seed(7)\n",
    "\n",
    "colour_adj = ['white', 'black', 'red', 'blue', 'grey', 'yellow', 'green', 'orange', 'brown', 'silver']\n",
    "other_adj = ['camouflage', 'cargo', 'commercial', 'fighter', 'light']\n",
    "noun_list = ['aircraft', 'drone', 'helicopter', 'jet', 'missile', 'plane']\n",
    "\n",
    "def extract_parts(phrase):\n",
    "    colours, others, nouns = [], [], []\n",
    "    for word in phrase.split():\n",
    "        w = word.strip(',')\n",
    "        if w in colour_adj:\n",
    "            colours.append(w)\n",
    "        if w in other_adj:\n",
    "            others.append(w)\n",
    "        if w in noun_list:\n",
    "            nouns.append(w)\n",
    "    return colours, others, nouns\n",
    "\n",
    "def make_col_ph(colours):\n",
    "    if len(colours)<=2:\n",
    "        return ' and '.join(colours)\n",
    "    return ', '.join(colours[:-1]) + ', and ' + colours[-1]\n",
    "\n",
    "def make_phrase(colours, others, nouns):\n",
    "    col_ph = make_col_ph(colours)\n",
    "    oth_ph = ' '.join(others)\n",
    "    nn_ph = ' '.join(nouns)  # by right only 1 noun, but make robust\n",
    "    parts = [col_ph, oth_ph, nn_ph]\n",
    "    parts = [seg.strip(' ') for seg in parts if seg]  # remove empty parts\n",
    "    return ' '.join(parts)\n",
    "\n",
    "def rephrase(phrase, p=0.8):\n",
    "    if random.random() < p:\n",
    "        c,o,n = extract_parts(phrase)\n",
    "        random.shuffle(c)\n",
    "        new_ph = make_phrase(c,o,n)\n",
    "        return new_ph\n",
    "    else:\n",
    "        return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33d901a5-97cc-4bc7-a7b9-9c9fd5772fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset, split):\n",
    "    # Preprocessing the datasets.\n",
    "    data = dataset[split]\n",
    "    # We need to tokenize inputs and targets.\n",
    "    column_names = data.column_names\n",
    "\n",
    "    # 6. Get the column names for input/target.\n",
    "    image_column = \"image_path\"\n",
    "    caption_column = \"caption\"\n",
    "    dataset_columns = (image_column, caption_column)\n",
    "\n",
    "    # Preprocessing the datasets.\n",
    "    # We need to tokenize input captions and transform the images.\n",
    "    def permanent_mapping(examples):\n",
    "        captions = list(examples[caption_column])\n",
    "        #text_inputs = tokenizer(captions, padding=\"max_length\", truncation=True)\n",
    "        #examples[\"input_ids\"] = text_inputs.input_ids\n",
    "        examples[\"input_ids\"] = captions\n",
    "        #examples[\"attention_mask\"] = text_inputs.attention_mask\n",
    "        return examples\n",
    "\n",
    "    def transform_data(examples):\n",
    "        images = [read_image(image_file, mode=ImageReadMode.RGB) for image_file in examples[image_column]]\n",
    "        examples[\"pixel_values\"] = [image_transformations(image) for image in images]\n",
    "        captions = [rephrase(caption) for caption in examples['input_ids']]\n",
    "        text_inputs = tokenizer(captions, padding=\"max_length\", truncation=True)\n",
    "        examples[\"input_ids\"] = text_inputs.input_ids\n",
    "        examples[\"attention_mask\"] = text_inputs.attention_mask\n",
    "        return examples\n",
    "\n",
    "    data = data.map(\n",
    "        function=permanent_mapping,\n",
    "        batched=True,\n",
    "        remove_columns=[col for col in column_names if col != image_column],\n",
    "        desc=f\"Running mapping on {split} dataset\",\n",
    "    )\n",
    "\n",
    "    # Transform images on the fly as doing it on the whole dataset takes too much time.\n",
    "    data.set_transform(transform_data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b43b9107-6368-46e1-8538-8a08c22c690a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'transform'=<function preprocess_dataset.<locals>.transform_data at 0x7f4a8bc93520> of the transform datasets.arrow_dataset.Dataset.set_format couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = preprocess_dataset(dataset, \"train\")\n",
    "eval_dataset = preprocess_dataset(dataset, \"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb9dde24-5e6f-4db1-acdc-38282d5efafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    input_ids = torch.tensor([example[\"input_ids\"] for example in examples], dtype=torch.long)\n",
    "    attention_mask = torch.tensor([example[\"attention_mask\"] for example in examples], dtype=torch.long)\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"return_loss\": True,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea68f5a8-74cd-4aa2-b0e6-082f3e1f06bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16575' max='16575' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16575/16575 5:54:24, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.135100</td>\n",
       "      <td>0.108649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.079500</td>\n",
       "      <td>0.068721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.070200</td>\n",
       "      <td>0.064778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.059200</td>\n",
       "      <td>0.057487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.047500</td>\n",
       "      <td>0.057833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize Trainer\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=0,\n",
    "    weight_decay=0.02,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    remove_unused_columns=False,\n",
    "    output_dir=\"roberta-beit\",\n",
    "    save_strategy = \"no\",\n",
    "    \n",
    "    num_train_epochs=5,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ebdbde6-94b3-4f21-9a4c-a14b3f1105dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3315' max='3315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3315/3315 1:43:10, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.042200</td>\n",
       "      <td>0.059576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize Trainer\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-6,\n",
    "    warmup_steps=0,\n",
    "    weight_decay=0.02,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    remove_unused_columns=False,\n",
    "    output_dir=\"roberta-beit\",\n",
    "    save_strategy = \"no\",\n",
    "    \n",
    "    num_train_epochs=1,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a053f00e-9411-468f-a752-ffe339b7f7fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='593' max='3315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 593/3315 18:04 < 1:23:16, 0.54 it/s, Epoch 0.18/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.041600</td>\n",
       "      <td>0.055503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    674\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/datasets/arrow_dataset.py:2865\u001b[0m, in \u001b[0;36mDataset.__getitems__\u001b[0;34m(self, keys)\u001b[0m\n\u001b[1;32m   2864\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2865\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2866\u001b[0m n_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch))])\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/datasets/arrow_dataset.py:2861\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2860\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2861\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/datasets/arrow_dataset.py:2846\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2845\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices)\n\u001b[0;32m-> 2846\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2847\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[1;32m   2848\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2849\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/datasets/formatting/formatting.py:633\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/datasets/formatting/formatting.py:401\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/datasets/formatting/formatting.py:516\u001b[0m, in \u001b[0;36mCustomFormatter.format_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    515\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_features_decoder\u001b[38;5;241m.\u001b[39mdecode_batch(batch)\n\u001b[0;32m--> 516\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 23\u001b[0m, in \u001b[0;36mpreprocess_dataset.<locals>.transform_data\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform_data\u001b[39m(examples):\n\u001b[0;32m---> 23\u001b[0m     images \u001b[38;5;241m=\u001b[39m [read_image(image_file, mode\u001b[38;5;241m=\u001b[39mImageReadMode\u001b[38;5;241m.\u001b[39mRGB) \u001b[38;5;28;01mfor\u001b[39;00m image_file \u001b[38;5;129;01min\u001b[39;00m examples[image_column]]\n\u001b[1;32m     24\u001b[0m     examples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [image_transformations(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n",
      "Cell \u001b[0;32mIn[11], line 23\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform_data\u001b[39m(examples):\n\u001b[0;32m---> 23\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\u001b[43mread_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mImageReadMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRGB\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m image_file \u001b[38;5;129;01min\u001b[39;00m examples[image_column]]\n\u001b[1;32m     24\u001b[0m     examples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [image_transformations(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torchvision/io/image.py:275\u001b[0m, in \u001b[0;36mread_image\u001b[0;34m(path, mode, apply_exif_orientation)\u001b[0m\n\u001b[1;32m    274\u001b[0m     _log_api_usage_once(read_image)\n\u001b[0;32m--> 275\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decode_image(data, mode, apply_exif_orientation\u001b[38;5;241m=\u001b[39mapply_exif_orientation)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torchvision/io/image.py:52\u001b[0m, in \u001b[0;36mread_file\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     51\u001b[0m     _log_api_usage_once(read_file)\n\u001b[0;32m---> 52\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/_ops.py:854\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(self_, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    847\u001b[0m     \u001b[38;5;66;03m# use `self_` to avoid naming collide with aten ops arguments that\u001b[39;00m\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;66;03m# named \"self\". This way, all the aten ops can be called by kwargs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 854\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mself_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [Errno 4] Interrupted system call: './cropped/2484.jpg'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 22\u001b[0m\n\u001b[1;32m      2\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      3\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m,\n\u001b[1;32m      4\u001b[0m     warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     16\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     17\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mcollate_fn,\n\u001b[1;32m     21\u001b[0m )\n\u001b[0;32m---> 22\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/trainer.py:1836\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1833\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1835\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1836\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1837\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1839\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/accelerate/data_loader.py:464\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    463\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_blocking)\n\u001b[0;32m--> 464\u001b[0m next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_batches:\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initialize Trainer\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-6,\n",
    "    warmup_steps=0,\n",
    "    weight_decay=0.1,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    remove_unused_columns=False,\n",
    "    output_dir=\"roberta-beit\",\n",
    "    save_strategy = \"no\",\n",
    "    \n",
    "    num_train_epochs=1,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e44dc3af-c5dc-4dc8-ac97-f24f34b0de21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.055503200739622116}\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c3adeaf-69f2-4412-b4c9-2a98caf03e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['caption_models/roberta-beit-v1-2/preprocessor_config.json']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = 'caption_models/roberta-beit-v1-2'\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "image_processor.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1fd6e28-86af-46ba-9154-0e7949815ff8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import VisionTextDualEncoderConfig\n",
    "vision_text_config = VisionTextDualEncoderConfig.from_pretrained(output_dir)\n",
    "model = VisionTextDualEncoderModel.from_pretrained(output_dir, config=vision_text_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "image_processor = AutoImageProcessor.from_pretrained(output_dir)\n",
    "processor = VisionTextDualEncoderProcessor(image_processor, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4f647ec-82f0-4984-bc03-6d36686bc075",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18942752-2687-4bd0-869d-480f639fd198",
   "metadata": {},
   "source": [
    "https://medium.com/@kerry.halupka/getting-started-with-openais-clip-a3b8f5277867\n",
    "https://huggingface.co/docs/transformers/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "287dd6f9-92c4-44a2-a20d-a0d600095c12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['white and orange light aircraft', 'black helicopter', 'black fighter plane', 'white missile']\n",
      "tensor([[1.0000, 0.3041, 0.0653, 0.7419]], device='cuda:0',\n",
      "       grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAkAFQDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDv/Esjto7lDgqwOfxx/WszwfrElxYGK5kyyswyT6Vq6tpbX9uyxzSI3GF3fKfwrmk06TTZDGsGA7kvlsZz39+9XO6kpIKc06bgzszdwjOZk4/2hULapZpkG4jBHbNcXqsg09Glig8zLBWGPWq0WsxeaztZEOgw33Tx+VTLEJOxtSwMqkeZHb/2zYF1QXUZZug3cmmQ63ptzM0MV7A0inBXeMg/SsrTJI715jBDiW2y2Noy2RWLcfD7T9Umkvj59pNPIWdUkwQc9fpVxrXV0YTw/I+WR3mc8jpRXP6F4fv9BjMEV891bnokxyVPsa1/tE6KWe3JHsR/WtVVj1MXSkWKKpf2rApxKrx/XB/lmrEN3b3BxHKp+vH86pTiyXCSJaKl8s+1FVzE2ZZ2p6CmvFE42sgI96i3mk3msrF3Ib2wsp4SsyqinvnFcrqWl2UGDbFJmwRhQSSMeo4rrnw4wwBHpQjbPugAfSolTuawrOJwcNzc21yGitrhZJF2nCELx6mppIr4sWaUxqefmOf5127sXUjOPcAVnX9tbeRvmt/P2885zUeyaNHX5mZumaffzuGa5Ij7k8Vvy2wEXk8OWGNxPJqqumzBkljmZEAyFVsg0+4t7iFzI8RcZwMH/wCvSSWwNvcrS20SRGKe3LL2YsTiobYwzzAW0LMBwx28A/X1rTCGTY5OwgD5RVhSFGFAA9qr2dyHUS1GrbFVA89l46UU/wAyir5DPnGUUUVoZCUUUUxgaSiikMcjtGflOKDdS3A2uRhTwAKKKl7lje9BooqiBKKKKYj/2Q==\n",
      "image/png": "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\n",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=84x36>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "image = [Image.open(dataset['train'][idx]['image_path'])]\n",
    "texts = [dataset['train'][idx]['caption'], \"black helicopter\", \"black fighter plane\", \"white missile\"]\n",
    "print(texts)\n",
    "inputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True).to(device)\n",
    "model = model.to(device)\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image.sigmoid() \n",
    "print(logits_per_image)\n",
    "image[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "410e8f4a-8a65-4ee8-8fdd-c59bbec1e225",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['green light aircraft', 'black helicopter', 'black fighter plane', 'white missile']\n",
      "4.247323751449585\n",
      "tensor([[1.0000, 0.1587, 0.2707, 0.2773]], device='cuda:0',\n",
      "       grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAcAHgDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDt5764BKg4Xt9KbDMS4Erkj1qe5iRoCAfmHIrPBxx2rqR49apUpVL30L9xEpO4HgDiqMFt50jMaesknlEDj2NXbbYsIPT1qk7I6aVWNTYpTR+QRtFUr24KoFxW7sWX3qhf2sTId2Fx3JqoyV9TSUdNDl3kJY80sWS2M9adNJYx3CxtOOTjC81uJpFlHCkstw3zDIUD5j+FburFHM9N2V7SFYTvJyauLeMWGzJ+lcT4g8TXOm34sbe12F+Vkc7uPWnW8epmRPN1gxrcJkMgxg/3fbrWDqc2yNo3tdHb3GoxWy/6XcRxD0Zufy61kzeKbUu0VlBNdyKM/Lwv59a5ezvtP0i4b7c7TyyOQZmGQAO9XLLUNKN7cy/a4Y97AAZxkAdf51C957g2WrufV9WiMDmOzgbrHHyx+ppbLwnp0a77mJpn7FzT7e+N1qHkaYFnCANJKxwij+prbRZmmCkjFaKMB6iWum29qoNraxR+6qAaK0IIJBncaKV0VYbaqzlvMOc9KrX9jP5xMN40SN0CoDj86tRsVjJHWmSfvYMPyM1MkZ1oc8bHLavef2Uqi51m5Ern5UjVckevSkstSs7gvEmr38zuBhnVVx+lbU2j6fcyGSa1jd/7xHNUJtJsIzujtVQ/7JI/rUpXMFSUY3W5nLeqZQkdzebt5UlpT1HH+P5VHLaC+m2S3NwxbcAfPOCR9O1Xl0623fcPb+I1sRaHYPECUkz7St/jTkrFqEn1MjRtOtNMaRgyjcAytIckAjpk1ammsvOeRr4hm67ZB+Qqa48N6WvzfZgT6sc/zpYNBsSV2oy/7rYqTKVB3vc5zUrXSL+RWmeWeRPunLMR+VUvsFsSVEFw4z3JH8zXfjQbNgMmX/vuqV7otnEo2h+ndqVtSlCSW5xMmj6aI2kbTenJZ2/+vUhstPt4lM1vbbyME7znNbMmn25cqVJHoWNWYtPtECt5CE56kZrRQ6goN7sPDljDaWskkcPlGU52j07f1rfgTgHqaq26jca1YlAUYHatGuVWOmKLMI2gUU1Cc0VmaH//2Q==\n",
      "image/png": "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\n",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=120x28>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "output_dir = 'caption_models/roberta-beit-v1-1'\n",
    "vision_text_config = VisionTextDualEncoderConfig.from_pretrained(output_dir)\n",
    "model = VisionTextDualEncoderModel.from_pretrained(output_dir, config=vision_text_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "image_processor = AutoImageProcessor.from_pretrained(output_dir)\n",
    "processor = VisionTextDualEncoderProcessor(image_processor, tokenizer)\n",
    "\n",
    "idx = 1\n",
    "image = [Image.open(dataset['train'][idx]['image_path'])]\n",
    "texts = [dataset['train'][idx]['caption'], \"black helicopter\", \"black fighter plane\", \"white missile\"]\n",
    "print(texts)\n",
    "inputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True).to(device)\n",
    "model = model.to(device)\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image.sigmoid() \n",
    "print(time()-start_time)\n",
    "print(logits_per_image)\n",
    "image[0]"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
