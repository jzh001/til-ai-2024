{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "125eb28a-dc3e-4917-9e57-ae8e548adc91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from preprocessing import createCroppedFolder\n",
    "\n",
    "#createCroppedFolder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff61c338-d803-4a1e-896e-de6dcac712c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# file_path = \"./vlm_caption.jsonl\"\n",
    "# df = pd.read_json(path_or_buf=file_path, lines=True)\n",
    "# df = df.rename(columns={'image_path': 'file_name'})\n",
    "# df['file_name'] = ['./'+ name[-4:]\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e756cb8-5c31-4f6c-a37a-4f8afd2e0c3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/envs/pytorch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: accelerate in /opt/conda/envs/pytorch/lib/python3.10/site-packages (0.30.1)\n",
      "Requirement already satisfied: transformers==4.37.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (4.37.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/envs/pytorch/lib/python3.10/site-packages (2.19.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.37.0) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.37.0) (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.37.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.37.0) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.37.0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.37.0) (2024.5.10)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.37.0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.37.0) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.37.0) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.37.0) (4.66.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from accelerate) (5.9.3)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from accelerate) (2.3.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (16.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.0) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers==4.37.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers==4.37.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers==4.37.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers==4.37.0) (2024.2.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/envs/pytorch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install accelerate transformers==4.37.0 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41481d29-59f2-44cc-8b8d-0affb815d846",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The projection layer and logit scale weights `['visual_projection.weight', 'text_projection.weight', 'logit_scale']` are newly initialized. You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from torchvision.io import ImageReadMode, read_image\n",
    "from torchvision.transforms import CenterCrop, ConvertImageDtype, Normalize, Resize, RandomHorizontalFlip,RandomAffine,GaussianBlur,RandomCrop\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    VisionTextDualEncoderModel,\n",
    "    VisionTextDualEncoderProcessor,\n",
    "    AutoTokenizer,\n",
    "    AutoImageProcessor\n",
    ")\n",
    "\n",
    "model = VisionTextDualEncoderModel.from_vision_text_pretrained(\n",
    "    \"google/vit-base-patch16-224\", \"FacebookAI/roberta-base\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "processor = VisionTextDualEncoderProcessor(image_processor, tokenizer)\n",
    "config = model.config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6a241e9-2223-4fba-aa29-47cba8833f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image_path', 'image_id', 'original_id', 'caption'],\n",
       "        num_rows: 22330\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image_path', 'image_id', 'original_id', 'caption'],\n",
       "        num_rows: 5583\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['image_path', 'image_id', 'original_id', 'caption'],\n",
       "        num_rows: 5583\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "file_path = \"./vlm_caption.jsonl\"\n",
    "dataset = load_dataset('json', data_files=file_path)\n",
    "dataset = dataset['train'].train_test_split(test_size=0.2, seed = 42)\n",
    "dataset['valid'] = dataset['test'] \n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2900a1bb-1ca5-4372-8fd0-3d1e9f4be143",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset.save_to_disk(\"saved_cropped_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4600b2b4-fe77-4f9d-9d36-6f720e5908a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from datasets import load_from_disk\n",
    "\n",
    "# dataset = load_from_disk(\"saved_cropped_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc8453e3-63e0-4d6a-8e21-538c1937f8fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_path': './cropped/44583.jpg',\n",
       " 'image_id': 44583,\n",
       " 'original_id': 2229,\n",
       " 'caption': 'blue and red light aircraft'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b567c72a-3301-455b-a585-6098b66ce597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use torchvision for faster image pre-processing. The transforms are implemented as nn.Module,\n",
    "# so we jit it to be faster.\n",
    "class Transform(torch.nn.Module):\n",
    "    def __init__(self, image_size, mean, std):\n",
    "        super().__init__()\n",
    "        self.transforms = torch.nn.Sequential(\n",
    "            Resize([image_size + 5], interpolation=InterpolationMode.BICUBIC),\n",
    "            RandomCrop(image_size),\n",
    "            ConvertImageDtype(torch.float),\n",
    "            RandomAffine((10)),\n",
    "            RandomHorizontalFlip(p=0.5),\n",
    "            #GaussianBlur(kernel_size=3),\n",
    "            Normalize(mean, std),\n",
    "        )\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        \"\"\"`x` should be an instance of `PIL.Image.Image`\"\"\"\n",
    "        with torch.no_grad():\n",
    "            x = self.transforms(x)\n",
    "        return x\n",
    "\n",
    "# For preprocessing the datasets.\n",
    "# Initialize torchvision transforms and jit it for faster processing.\n",
    "image_transformations = Transform(\n",
    "    config.vision_config.image_size, image_processor.image_mean, image_processor.image_std\n",
    ")\n",
    "image_transformations = torch.jit.script(image_transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33d901a5-97cc-4bc7-a7b9-9c9fd5772fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset, split):\n",
    "    # Preprocessing the datasets.\n",
    "    data = dataset[split]\n",
    "    # We need to tokenize inputs and targets.\n",
    "    column_names = data.column_names\n",
    "\n",
    "    # 6. Get the column names for input/target.\n",
    "    image_column = \"image_path\"\n",
    "    caption_column = \"caption\"\n",
    "    dataset_columns = (image_column, caption_column)\n",
    "\n",
    "    # Preprocessing the datasets.\n",
    "    # We need to tokenize input captions and transform the images.\n",
    "    def tokenize_captions(examples):\n",
    "        captions = list(examples[caption_column])\n",
    "        text_inputs = tokenizer(captions, padding=\"max_length\", truncation=True)\n",
    "        examples[\"input_ids\"] = text_inputs.input_ids\n",
    "        examples[\"attention_mask\"] = text_inputs.attention_mask\n",
    "        return examples\n",
    "\n",
    "    def transform_images(examples):\n",
    "        images = [read_image(image_file, mode=ImageReadMode.RGB) for image_file in examples[image_column]]\n",
    "        examples[\"pixel_values\"] = [image_transformations(image) for image in images]\n",
    "        return examples\n",
    "\n",
    "    data = data.map(\n",
    "        function=tokenize_captions,\n",
    "        batched=True,\n",
    "        remove_columns=[col for col in column_names if col != image_column],\n",
    "        desc=f\"Running tokenizer on {split} dataset\",\n",
    "    )\n",
    "\n",
    "    # Transform images on the fly as doing it on the whole dataset takes too much time.\n",
    "    data.set_transform(transform_images)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b43b9107-6368-46e1-8538-8a08c22c690a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'transform'=<function preprocess_dataset.<locals>.transform_images at 0x7fe779962c20> of the transform datasets.arrow_dataset.Dataset.set_format couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = preprocess_dataset(dataset, \"train\")\n",
    "eval_dataset = preprocess_dataset(dataset, \"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb9dde24-5e6f-4db1-acdc-38282d5efafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    input_ids = torch.tensor([example[\"input_ids\"] for example in examples], dtype=torch.long)\n",
    "    attention_mask = torch.tensor([example[\"attention_mask\"] for example in examples], dtype=torch.long)\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"return_loss\": True,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea68f5a8-74cd-4aa2-b0e6-082f3e1f06bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16752' max='16752' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16752/16752 6:38:01, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.178300</td>\n",
       "      <td>0.125299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.088200</td>\n",
       "      <td>0.089452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.064200</td>\n",
       "      <td>0.072631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.061200</td>\n",
       "      <td>0.066289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.059100</td>\n",
       "      <td>0.065167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>0.060200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize Trainer\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=0,\n",
    "    weight_decay=0.02,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    remove_unused_columns=False,\n",
    "    output_dir=\"roberta-vit\",\n",
    "    save_strategy = \"no\",\n",
    "    \n",
    "    num_train_epochs=6,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e44dc3af-c5dc-4dc8-ac97-f24f34b0de21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='698' max='698' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [698/698 06:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06436754018068314, 'eval_runtime': 365.9573, 'eval_samples_per_second': 15.256, 'eval_steps_per_second': 1.907, 'epoch': 6.0}\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c3adeaf-69f2-4412-b4c9-2a98caf03e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['caption_models/roberta-vit-v1-1/preprocessor_config.json']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = 'caption_models/roberta-vit-v1-1'\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "image_processor.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1fd6e28-86af-46ba-9154-0e7949815ff8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import VisionTextDualEncoderConfig\n",
    "vision_text_config = VisionTextDualEncoderConfig.from_pretrained(output_dir)\n",
    "model = VisionTextDualEncoderModel.from_pretrained(output_dir, config=vision_text_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "image_processor = AutoImageProcessor.from_pretrained(output_dir)\n",
    "processor = VisionTextDualEncoderProcessor(image_processor, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4f647ec-82f0-4984-bc03-6d36686bc075",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18942752-2687-4bd0-869d-480f639fd198",
   "metadata": {},
   "source": [
    "https://medium.com/@kerry.halupka/getting-started-with-openais-clip-a3b8f5277867\n",
    "https://huggingface.co/docs/transformers/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "287dd6f9-92c4-44a2-a20d-a0d600095c12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['blue and red light aircraft', 'black helicopter', 'black fighter plane', 'white missile']\n",
      "tensor([[1.0000, 0.0776, 0.1140, 0.0228]], device='cuda:0',\n",
      "       grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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\n",
      "image/png": "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\n",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=84x36>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "image = [Image.open(dataset['train'][idx]['image_path'])]\n",
    "texts = [dataset['train'][idx]['caption'], \"black helicopter\", \"black fighter plane\", \"white missile\"]\n",
    "print(texts)\n",
    "inputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True).to(device)\n",
    "model = model.to(device)\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image.sigmoid() \n",
    "print(logits_per_image)\n",
    "image[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "410e8f4a-8a65-4ee8-8fdd-c59bbec1e225",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['white, black, and grey missile', 'black helicopter', 'black fighter plane', 'white missile']\n",
      "3.69270920753479\n",
      "tensor([[1.0000, 0.9554, 0.8033, 0.9774]], device='cuda:0',\n",
      "       grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCABIACQDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDWpQKQU7FegcYU1nVFLMcADJNOp1rELzUYrUjcgG+XJ6LUzlyq7HFXdiHyJHAfaCHAZeegxRVq7WKS6kaIbY84UZ7UVj7xehBSiilxW5mMmlSCF5ZDhEGTWfol/qVreXAubcx/bBlWYcqMdPSrSRHVNct9PTmOIiWf3x0X+v5VtazKj3awxgYhGMgd6wk+aaiaxXLG5n59v1opcUV0GQ0VFeXcdjZy3MpwkakmpwK4L4jaqwig0i3YmWZgWAPXsBWc5WVyoq7sP8J+JNRi1yW6CiQXD42MBnn0PWu65YlmOWJyx9TXJ+GNKitpYogu5rWJQ8nq57fl/Ouu96ikvtFVHrYbRTqK2MyOeZLe3kmkbCIpYn0xXlWms2ueI7nWpgWiiJZAe56KP8+tdV4/1CSPSV061yZ7o4IHUIOtReGdKWAWtltH7sefOfU/wj/PpXPUfM+U2hornTaTZmzsVV/9a/zyH/aNXh1pcYoFbJWVjF6u4lFOxRTA5DxN4durzUI9SilB8tNoQE/LjnPStfw3Yy2mlo91j7TKAXPf2zRRWUUuY1exs4ooorQyDFFFFMD/2Q==\n",
      "image/png": "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\n",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=36x72>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "output_dir = 'caption_models/roberta-vit-v1-1'\n",
    "vision_text_config = VisionTextDualEncoderConfig.from_pretrained(output_dir)\n",
    "model = VisionTextDualEncoderModel.from_pretrained(output_dir, config=vision_text_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "image_processor = AutoImageProcessor.from_pretrained(output_dir)\n",
    "processor = VisionTextDualEncoderProcessor(image_processor, tokenizer)\n",
    "\n",
    "idx = 1\n",
    "image = [Image.open(dataset['train'][idx]['image_path'])]\n",
    "texts = [dataset['train'][idx]['caption'], \"black helicopter\", \"black fighter plane\", \"white missile\"]\n",
    "print(texts)\n",
    "inputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True).to(device)\n",
    "model = model.to(device)\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image.sigmoid() \n",
    "print(time()-start_time)\n",
    "print(logits_per_image)\n",
    "image[0]"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
